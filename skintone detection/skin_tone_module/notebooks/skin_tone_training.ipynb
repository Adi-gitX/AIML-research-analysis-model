{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0 — Setup: clone STONE and install dependencies (run in Colab)\n",
    "# If you already installed, you can skip cloning.\n",
    "!git clone https://github.com/ChenglongMa/SkinToneClassifier.git --depth 1\n",
    "%cd SkinToneClassifier\n",
    "!pip install .[all] torch torchvision opencv-python-headless tqdm scikit-learn matplotlib colorutils --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9aee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workspace directories under /content/data\n",
    "import os\n",
    "base = '/content/data'\n",
    "for d in ['raw_faces','skin_patches','dataset','model','feedback']:\n",
    "    os.makedirs(os.path.join(base, d), exist_ok=True)\n",
    "print('✅ Project structure created:', os.listdir(base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559b3bf",
   "metadata": {},
   "source": [
    "Upload 20–100 portrait images into `/content/data/raw_faces/`. In Colab: use the left sidebar Files upload control or mount Google Drive and copy files there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b453f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 — Data extraction using STONE\n",
    "import os, json, cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "# STONE's process function is available after installing the package\n",
    "from stone.api import process\n",
    "\n",
    "input_dir = '/content/data/raw_faces/'\n",
    "output_dir = '/content/data/skin_patches/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dataset = []\n",
    "\n",
    "for img_path in tqdm(sorted(glob(input_dir + '*'))):\n",
    "    if not img_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        continue\n",
    "    try:\n",
    "        result = process(img_path, image_type='color', palette='perla')\n",
    "        faces = result.get('faces', [])\n",
    "        for i, face in enumerate(faces):\n",
    "            tone = face.get('tone_label') or face.get('tone') or 'unknown'\n",
    "            hexcol = face.get('skin_tone')\n",
    "            crop = face.get('report_image')  # numpy BGR image from STONE\n",
    "            if crop is not None:\n",
    "                name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "                save_path = os.path.join(output_dir, f'{name}_{i}_{tone}.jpg')\n",
    "                # STONE's report_image is usually RGB; convert to BGR if needed for cv2.imwrite\n",
    "                try:\n",
    "                    cv2.imwrite(save_path, cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))\n",
    "                except Exception:\n",
    "                    cv2.imwrite(save_path, crop)\n",
    "                dataset.append({'img': save_path, 'tone': tone, 'hex': hexcol})\n",
    "    except Exception as e:\n",
    "        print('⚠️', img_path, e)\n",
    "\n",
    "json.dump(dataset, open('/content/tone_dataset_raw.json','w'), indent=2)\n",
    "print('✅ Extracted', len(dataset), 'skin patches to', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580ce99",
   "metadata": {},
   "source": [
    "## STEP 2 — Clean & re-label to Monk Skin Tone (MST) 1–10\n",
    "STONE labels vary by palette — we map them to a 10-level MST scale. After auto-mapping, manually inspect and correct samples in `/content/data/dataset/` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f06d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map STONE tone labels to MST 1-10 (example mapping; adjust after inspection)\n",
    "import json, os, shutil\n",
    "tone_map = {\n",
    "    'tone_A': 'tone_1', 'tone_B': 'tone_2', 'tone_C': 'tone_3', 'tone_D': 'tone_4',\n",
    "    'tone_E': 'tone_5', 'tone_F': 'tone_6', 'tone_G': 'tone_7', 'tone_H': 'tone_8',\n",
    "    'tone_I': 'tone_9', 'tone_J': 'tone_10'\n",
    "}\n",
    "data = json.load(open('/content/tone_dataset_raw.json')) if os.path.exists('/content/tone_dataset_raw.json') else []\n",
    "for d in data:\n",
    "    t = d.get('tone', '')\n",
    "    mapped = tone_map.get(t, 'tone_5')\n",
    "    dest = os.path.join('/content/data/dataset', mapped)\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    try:\n",
    "        shutil.copy(d['img'], dest)\n",
    "    except Exception as e:\n",
    "        print('copy error', d['img'], e)\n",
    "print('✅ Dataset grouped by MST scale under /content/data/dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71afa9d",
   "metadata": {},
   "source": [
    "## STEP 3 — Train EfficientNet-B0 classifier\n",
    "This trains a classifier on `/content/data/dataset/`. For a real project, create a train/val split and use more epochs, balanced classes, and augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0da96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (simplified)\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "])\n",
    "dataset = datasets.ImageFolder('/content/data/dataset', transform=transform)\n",
    "if len(dataset) == 0:\n",
    "    raise SystemExit('No training data found in /content/data/dataset — please run extraction and relabeling first')\n",
    "# small train/val split\n",
    "val_size = int(0.15 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
    "classes = dataset.classes\n",
    "print('Classes:', classes)\n",
    "\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "# EfficientNet-B0 classifier head location may differ between torchvision versions; adjust if needed\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, len(classes))\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    epoch_loss = running_loss / max(len(train_loader.dataset), 1)\n",
    "    # validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = model(imgs).argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "    acc = correct / max(total, 1)\n",
    "    print(f'Epoch {epoch+1}/10 loss={epoch_loss:.4f} val_acc={acc:.4f}')\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        os.makedirs('/content/data/model', exist_ok=True)\n",
    "        torch.save(model.state_dict(), '/content/data/model/skin_tone_cnn.pt')\n",
    "        print('Saved best model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68c88c",
   "metadata": {},
   "source": [
    "## STEP 4 — Tone → Fashion Palette Mapping\n",
    "Save a JSON mapping from tone label → recommended color palette. You can expand this into a more advanced rule engine later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fashion_palette = {\n",
    "    'tone_1':['navy','silver','lavender'],\n",
    "    'tone_2':['skyblue','gray','rose'],\n",
    "    'tone_3':['tan','denim','burgundy'],\n",
    "    'tone_4':['olive','coral','beige'],\n",
    "    'tone_5':['chocolate','cream','gold'],\n",
    "    'tone_6':['emerald','taupe','rust'],\n",
    "    'tone_7':['sand','peach','khaki'],\n",
    "    'tone_8':['maroon','bronze','ivory'],\n",
    "    'tone_9':['darkolive','teal','white'],\n",
    "    'tone_10':['black','gold','red']\n",
    "}\n",
    "json.dump(fashion_palette, open('/content/fashion_palette.json','w'), indent=2)\n",
    "print('✅ Palette mapping saved to /content/fashion_palette.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Predict example and fetch palette\n",
    "from PIL import Image\n",
    "import torch, json\n",
    "model.eval()\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "])\n",
    "classes = dataset.classes\n",
    "def predict_tone(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = transform_eval(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).argmax(1).item()\n",
    "    return classes[pred]\n",
    "\n",
    "from glob import glob\n",
    "test_files = glob('/content/data/skin_patches/*.jpg')\n",
    "if len(test_files) > 0:\n",
    "    test_img = test_files[0]\n",
    "    tone_pred = predict_tone(test_img)\n",
    "    palette = json.load(open('/content/fashion_palette.json'))\n",
    "    print('Predicted:', tone_pred)\n",
    "    print('Recommended colors:', palette.get(tone_pred, ['black','white']))\n",
    "else:\n",
    "    print('No skin patches found in /content/data/skin_patches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117381b",
   "metadata": {},
   "source": [
    "## STEP 5 — Feedback & Retraining\n",
    "Collect user feedback (like/dislike) for predictions and store it in a feedback JSON; periodically merge correct feedback into training data and fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "feedback_path = '/content/data/feedback/feedback.json'\n",
    "def add_feedback(img, tone, correct=True):\n",
    "    fb = []\n",
    "    if os.path.exists(feedback_path):\n",
    "        fb = json.load(open(feedback_path))\n",
    "    fb.append({'img': img, 'tone': tone, 'correct': correct})\n",
    "    json.dump(fb, open(feedback_path, 'w'), indent=2)\n",
    "    print('✅ Feedback stored')\n",
    "\n",
    "# Example usage (mark incorrect prediction)\n",
    "# add_feedback(test_img, tone_pred, correct=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3521ca",
   "metadata": {},
   "source": [
    "## Next-level improvements\n",
    "- Lighting invariance: add gray-world or Shades-of-Gray normalization before cropping/training.\n",
    "- Undertone detection: add a small LAB-based head (a/b channels) as a multitask label.\n",
    "- Bias reduction: ensure balanced samples per MST label; use oversampling/augmentation.\n",
    "- Serving: export model via TorchScript or ONNX and serve with FastAPI.\n",
    "\n",
    "---\n",
    "You can download this notebook as `.ipynb` and open it directly in Colab."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
